{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AdaptiveEpsilon:\n",
    "    \"\"\"修正后的自适应epsilon策略\"\"\"\n",
    "    def __init__(self, start=1.0, end=0.05, base_decay=0.999):\n",
    "        self.current_value = start\n",
    "        self.end = end\n",
    "        self.base_decay = base_decay\n",
    "        self.td_error_history = deque(maxlen=50)  # TD误差记录窗口\n",
    "        self.min_samples = 10  # 开始自适应前的最小样本数\n",
    "    \n",
    "    def update(self, td_error):\n",
    "        # 确保td_error是合理值\n",
    "        if not np.isfinite(td_error) or abs(td_error) > 1000:\n",
    "            td_error = 0.0  # 重置异常值\n",
    "        \n",
    "        # 记录当前TD误差\n",
    "        self.td_error_history.append(td_error)\n",
    "        \n",
    "        # 样本不足时使用基础衰减\n",
    "        if len(self.td_error_history) < self.min_samples:\n",
    "            self.current_value = max(self.end, self.current_value * self.base_decay)\n",
    "            return self.current_value\n",
    "        \n",
    "        # 计算TD误差的标准差(稳定性度量)\n",
    "        error_std = np.std(self.td_error_history)\n",
    "        \n",
    "        # 动态调整衰减因子 - 稳定性越低(波动越大)则探索应该越多\n",
    "        # 稳定性低时: 减缓衰减，保持较高探索率\n",
    "        # 稳定性高时: 加速衰减，减少探索\n",
    "        adaptive_factor = max(0.5, min(2.0, 1.0 + error_std))\n",
    "        \n",
    "        # 计算本次衰减量 - 确保decay < 1\n",
    "        decay = self.base_decay * (1.0 / adaptive_factor)\n",
    "        \n",
    "        # 应用指数衰减\n",
    "        self.current_value = max(self.end, self.current_value * decay)\n",
    "        \n",
    "        return self.current_value\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.current_value\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\" 深度Q网络 \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LayerNorm(512),  # 如果batch size为1，则使用LayerNorm\n",
    "            nn.LeakyReLU(0.01),   # 负斜率为0.01\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.LeakyReLU(0.01),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.LeakyReLU(0.01),\n",
    "\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # 使用He初始化 (LeakyReLU)\n",
    "                nn.init.kaiming_normal_(m.weight, a=0.01, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            # 输出层的初始化可以单独设置，但通常不需要特别设置\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer():\n",
    "    \"\"\" 优先级经验回放缓冲区 \"\"\"\n",
    "    def __init__(self, capacity, alpha=0.6, beta=0.4, beta_increment=0.001):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "        self.alpha = alpha  # 优先级指数\n",
    "        self.beta = beta    # 重要性采样权重\n",
    "        self.beta_increment = beta_increment\n",
    "        self.max_priority = 1.0  # 初始最大优先级\n",
    "        self.position = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # 新样本初始优先级设为当前最大优先级\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "            self.priorities.append(None)\n",
    "        \n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.priorities[self.position] = self.max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        # 计算采样概率\n",
    "        priorities = np.array(self.priorities, dtype=np.float32)\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        # 采样索引\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        \n",
    "        # 计算重要性采样权重\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()  # 归一化\n",
    "        \n",
    "        # 更新beta\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        \n",
    "        # 获取样本\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "        states = [item[0] for item in batch]\n",
    "        actions = [item[1] for item in batch]\n",
    "        rewards = [0.0 if r is None else r for _, _, r, _, _ in batch]\n",
    "        next_states = [item[3] for item in batch]\n",
    "        dones = [item[4] for item in batch]\n",
    "        \n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "            indices,\n",
    "            np.array(weights, dtype=np.float32)\n",
    "        )\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"更新样本优先级\"\"\"\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "            if priority > self.max_priority:\n",
    "                self.max_priority = priority\n",
    "\n",
    "\n",
    "class DQNAgent():\n",
    "    \"\"\" 包含经验回放和目标网络的DQN智能体 \"\"\"\n",
    "    def __init__(self, input_dim, action_dim, device=None):\n",
    "        # 直接接收输入维度和动作维度\n",
    "        self.input_dim = input_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # 设置计算设备\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # 超参数\n",
    "        self.gamma = 0.95   # 折扣因子\n",
    "        self.lr = 0.0005\n",
    "        self.batch_size = 256\n",
    "        self.sync_interval = 20  # 目标网络更新频率\n",
    "        self.buffer_capacity = 30000\n",
    "        self.update_frequency = 1  # 每5步更新一次网络\n",
    "\n",
    "        # 使用自适应epsilon策略替代固定衰减\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.05\n",
    "        self.epsilon_decay = 0.9995 \n",
    "        self.epsilon_strategy = AdaptiveEpsilon(start=self.epsilon_start, \n",
    "                                                end=self.epsilon_end, \n",
    "                                                base_decay=self.epsilon_decay)\n",
    "        \n",
    "        # 网络和优化器\n",
    "        self.online_net = DQN(self.input_dim, self.action_dim).to(self.device)\n",
    "        self.target_net = DQN(self.input_dim, self.action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        self.target_net.eval()  # 目标网络不计算梯度\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.online_net.parameters(), lr=self.lr)\n",
    "        \n",
    "        # 添加学习率调度器\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, \n",
    "            mode='min', \n",
    "            factor=0.2, \n",
    "            min_lr=1e-6,\n",
    "            patience=20, \n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # 使用优先级经验回放缓冲区\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(self.buffer_capacity)\n",
    "        \n",
    "        # 训练参数\n",
    "        self.epsilon = self.epsilon_start\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\" ε-greedy策略选择动作，使用自适应epsilon \"\"\"\n",
    "        # 获取当前epsilon值\n",
    "        epsilon = self.epsilon_strategy.value\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)   # 随机探索\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "                q_values = self.online_net(state_tensor)\n",
    "                return q_values.argmax().item()   # 选择最优动作\n",
    "    \n",
    "    def get_greedy_action(self, state):\n",
    "        \"\"\"获取贪婪策略下的动作（用于评估）\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "            q_values = self.online_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def sync_target_net(self):\n",
    "        \"\"\" 更新目标网络 \"\"\"\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\" 使用优先级经验回放更新网络 \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return 0, 0  # 返回损失和平均优先级\n",
    "        \n",
    "        # 从缓冲区采样（包含权重）\n",
    "        states, actions, rewards, next_states, dones, indices, weights = self.replay_buffer.get_batch(self.batch_size)\n",
    "        \n",
    "        # 转换为张量并移至设备\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        weights = torch.FloatTensor(weights).to(self.device)\n",
    "        \n",
    "        # 计算当前Q值\n",
    "        current_q_values = self.online_net(states)\n",
    "        # 使用gather获取选择的动作对应的Q值\n",
    "        current_q = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # 计算目标Q值（使用目标网络）\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states)\n",
    "            # 计算最大Q值\n",
    "            max_next_q = next_q_values.max(1)[0]\n",
    "            # 计算目标值\n",
    "            target_q = rewards + (1 - dones) * self.gamma * max_next_q\n",
    "        \n",
    "        # 计算TD误差（用于优先级）\n",
    "        td_errors = target_q - current_q\n",
    "        \n",
    "       # 使用TD误差更新自适应epsilon策略\n",
    "        avg_td_error = td_errors.abs().mean().item()\n",
    "        new_epsilon = self.epsilon_strategy.update(avg_td_error)\n",
    "        \n",
    "        # 计算损失（改为MSE损失）\n",
    "        # 原代码: loss = F.smooth_l1_loss(current_q, target_q, reduction='none')\n",
    "        loss = F.mse_loss(current_q, target_q, reduction='none')\n",
    "        loss = (loss * weights).mean()\n",
    "        \n",
    "        # 优化\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_net.parameters(), max_norm=1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 更新学习率\n",
    "        self.scheduler.step(loss)\n",
    "        \n",
    "        # 更新缓冲区优先级\n",
    "        priorities = td_errors.detach().abs().cpu().numpy() + 1e-5  # 避免零优先级\n",
    "        self.replay_buffer.update_priorities(indices, priorities)\n",
    "        \n",
    "        return loss.item(), priorities.mean()\n",
    "\n",
    "\n",
    "def enhanced_one_hot_encode(state, height, width, goal_state):\n",
    "    \"\"\" 增强状态表示 - 包含目标方向信息和相对位置 \"\"\"\n",
    "    base_dim = height * width\n",
    "    vec = np.zeros(base_dim + 6, dtype=np.float32)  # +6 表示额外特征\n",
    "    \n",
    "    # 基础状态编码\n",
    "    row, col = state\n",
    "    vec[row * width + col] = 1\n",
    "    \n",
    "    # 添加目标位置信息\n",
    "    goal_row, goal_col = goal_state\n",
    "    \n",
    "    # 相对位置特征\n",
    "    vec[base_dim] = (col - goal_col) / width  # 水平相对位置\n",
    "    vec[base_dim + 1] = (row - goal_row) / height  # 垂直相对位置\n",
    "    \n",
    "    # 方向特征\n",
    "    if col < goal_col:\n",
    "        vec[base_dim + 2] = 1  # 目标在右侧\n",
    "    elif col > goal_col:\n",
    "        vec[base_dim + 3] = 1  # 目标在左侧\n",
    "    \n",
    "    if row < goal_row:\n",
    "        vec[base_dim + 4] = 1  # 目标在上方\n",
    "    elif row > goal_row:\n",
    "        vec[base_dim + 5] = 1  # 目标在下方\n",
    "    \n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188712a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境地址: e:\\MyProjects\\rl_Q\\common_expand\n",
      "数据保存位置: ./Enhanced_DQN_data\n",
      "Training on: CUDA\n",
      "Episode    0, Reward:   0.00, Epsilon: 0.0500, Avg Reward (100):   0.00, Avg Loss: 0.0508\n",
      "Episode  100, Reward:   0.00, Epsilon: 0.0500, Avg Reward (100):  -0.30, Avg Loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 定期更新网络\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_count \u001b[38;5;241m%\u001b[39m agent\u001b[38;5;241m.\u001b[39mupdate_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mreplay_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m---> 66\u001b[0m     loss, avg_priority \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# 只记录有效的损失值\u001b[39;00m\n\u001b[0;32m     68\u001b[0m         loss_history\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[1;32mIn[3], line 237\u001b[0m, in \u001b[0;36mDQNAgent.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# 返回损失和平均优先级\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# 从缓冲区采样（包含权重）\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m states, actions, rewards, next_states, dones, indices, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# 转换为张量并移至设备\u001b[39;00m\n\u001b[0;32m    240\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(states)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[3], line 119\u001b[0m, in \u001b[0;36mPrioritizedReplayBuffer.get_batch\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    117\u001b[0m priorities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriorities, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    118\u001b[0m probs \u001b[38;5;241m=\u001b[39m priorities \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[1;32m--> 119\u001b[0m probs \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# 采样索引\u001b[39;00m\n\u001b[0;32m    122\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer), batch_size, p\u001b[38;5;241m=\u001b[39mprobs)\n",
      "File \u001b[1;32md:\\ProgramDev\\py_envs\\rl_pyenv\\lib\\site-packages\\numpy\\core\\_methods.py:47\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_prod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, sys, time\n",
    "\n",
    "# 保存位置\n",
    "traning_data_root = \"./Enhanced_DQN_data\"\n",
    "if not os.path.exists(traning_data_root):\n",
    "    os.makedirs(traning_data_root)\n",
    "\n",
    "if \"common_expand\" not in sys.path:\n",
    "    sys.path.append(os.path.join(os.getcwd(), \"common_expand\"))\n",
    "    \n",
    "print(\"环境地址:\", os.path.join(os.getcwd(), \"common_expand\"))\n",
    "print(\"数据保存位置:\", traning_data_root)\n",
    "\n",
    "from common_expand.gridworld import GridWorld\n",
    "\n",
    "# 创建网格世界环境\n",
    "env = GridWorld()\n",
    "height, width = env.height, env.width\n",
    "\n",
    "# 计算增强状态的维度\n",
    "base_dim = height * width\n",
    "state_dim = base_dim + 6  # 基础状态 + 6个方向特征\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device.type.upper()}\")\n",
    "\n",
    "# 创建智能体 - 使用正确的输入维度\n",
    "agent = DQNAgent(input_dim=state_dim, action_dim=len(env.action_space), device=device)\n",
    "\n",
    "# 训练参数\n",
    "episodes = 2000\n",
    "avg_episode = episodes // 20\n",
    "episode_rewards = []   # 记录每回合的奖励\n",
    "loss_history = []      # 记录损失函数\n",
    "epsilon_history = []   # 记录ε值的衰减\n",
    "steps_history = []     # 记录完成每回合的步数\n",
    "priority_history = []  # 记录优先级平均值\n",
    "training_data_file = os.path.join(traning_data_root, \"dqn_train_data.csv\")\n",
    "\n",
    "# 开始时间\n",
    "start_time = time.time()\n",
    "\n",
    "# 训练过程\n",
    "for episode in range(episodes):\n",
    "    # 重置环境并获取增强状态\n",
    "    raw_state = env.reset()\n",
    "    state = enhanced_one_hot_encode(raw_state, height, width, env.goal_state)\n",
    "    \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    max_steps = 300\n",
    "    \n",
    "    while not done and step_count < max_steps:        \n",
    "        # 选择并执行动作\n",
    "        action = agent.get_action(state)\n",
    "        raw_next_state, reward, done = env.step(action)\n",
    "        next_state = enhanced_one_hot_encode(raw_next_state, height, width, env.goal_state)\n",
    "        \n",
    "        # 存储经验\n",
    "        agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # 定期更新网络\n",
    "        if step_count % agent.update_frequency == 0 and len(agent.replay_buffer) >= agent.batch_size:\n",
    "            loss, avg_priority = agent.update()\n",
    "            if loss > 0:  # 只记录有效的损失值\n",
    "                loss_history.append(loss)\n",
    "                priority_history.append(avg_priority)\n",
    "        \n",
    "        # 定期更新目标网络\n",
    "        if step_count % agent.sync_interval == 0:\n",
    "            agent.sync_target_net()\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "    \n",
    "    if episode+1 == 500:\n",
    "        print(\"添加惩罚状态(12, 16): -1.0\")\n",
    "        env.add_pits((12, 16))\n",
    "\n",
    "    # 记录\n",
    "    episode_rewards.append(total_reward)\n",
    "    epsilon_history.append(agent.epsilon_strategy.value)\n",
    "    steps_history.append(step_count)\n",
    "    \n",
    "    # 保存训练数据\n",
    "    if not os.path.exists(training_data_file):\n",
    "        with open(training_data_file, \"w\") as f:\n",
    "            f.write(\"episode,total_reward,step_count,loss,epsilon,priority\\n\")\n",
    "    \n",
    "    with open(training_data_file, \"a\") as f:\n",
    "        avg_priority = priority_history[-1] if priority_history else 0\n",
    "        current_epsilon = agent.epsilon_strategy.value  # 获取当前自适应epsilon值\n",
    "        f.write(f\"{episode},{total_reward},{step_count},{loss},{current_epsilon},{avg_priority}\\n\")\n",
    "    \n",
    "    # 打印训练进度\n",
    "    if episode % avg_episode == 0 or episode == episodes - 1:\n",
    "        # 计算最近avg_episode个episode的平均奖励\n",
    "        window_size = min(avg_episode, len(episode_rewards))\n",
    "        avg_reward = np.mean(episode_rewards[-window_size:]) if window_size > 0 else 0\n",
    "        \n",
    "        # 计算最近avg_episode次更新的平均损失\n",
    "        window_size = min(avg_episode, len(loss_history))\n",
    "        avg_loss = np.mean(loss_history[-window_size:]) if window_size > 0 else 0\n",
    "        \n",
    "    if episode % avg_episode == 0 or episode == episodes - 1:\n",
    "        epsilon_val = min(10.0, max(0.001, agent.epsilon_strategy.value))  # 限制打印范围\n",
    "        print(f\"Episode {episode:4d}, Reward: {total_reward:6.2f}, \"\n",
    "              f\"Epsilon: {epsilon_val:.4f}, \"\n",
    "              f\"Avg Reward ({avg_episode}): {avg_reward:6.2f}, \"\n",
    "              f\"Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "duration_time = time.time() - start_time\n",
    "\n",
    "# 训练结束后保存最终模型\n",
    "torch.save({\n",
    "    'online_net_state_dict': agent.online_net.state_dict(),\n",
    "    'target_net_state_dict': agent.target_net.state_dict(),\n",
    "    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "    'epsilon_strategy': agent.epsilon_strategy,  # 保存自适应epsilon策略\n",
    "    'episode': episode,\n",
    "    'rewards_history': episode_rewards,\n",
    "    'loss_history': loss_history\n",
    "}, os.path.join(traning_data_root, 'dqn_gridworld_model_final.pth'))\n",
    "\n",
    "print(f\"Training completed! Model saved, duration time: {duration_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0787a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 绘制奖励变化图\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(episode_rewards, color=\"black\")\n",
    "plt.title('Total Reward per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()   # REVIEW: 导出为py文件, 改成plt.savefig(\"01_episode_rewards.png\")\n",
    "\n",
    "# 绘制每回合所花费的步数\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(steps_history, color=\"black\")\n",
    "plt.title('Steps per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Step')\n",
    "plt.show()   # REVIEW: 导出为py文件, 改成plt.savefig(\"02_steps_per_episode.png\")\n",
    "\n",
    "# 绘制损失变化图\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(loss_history, color=\"black\")\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()   # REVIEW: 导出为py文件, 改成plt.savefig(\"03_loss_per_episode.png\")\n",
    "\n",
    "# 绘制epsilon变化图\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(epsilon_history, color=\"black\")\n",
    "plt.title('Epsilon Decay')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.show()   # REVIEW: 导出为py文件, 改成plt.savefig(\"04_epsilon_decay.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "\n",
    "# 获取所有可能的状态\n",
    "for state in env.states():\n",
    "    # 跳过墙壁状态\n",
    "    if state in env.wall_state:\n",
    "        continue\n",
    "    # 计算增强编码的状态\n",
    "    encoded_state = enhanced_one_hot_encode(state, height, width, env.goal_state)\n",
    "    # 转换为Tensor并移动到正确设备\n",
    "    state_tensor = torch.FloatTensor(encoded_state).to(agent.device)\n",
    "    # 获取该状态下的所有动作Q值\n",
    "    with torch.no_grad():\n",
    "        q_values = agent.online_net(state_tensor).cpu().numpy()\n",
    "    # 存储所有动作的Q值\n",
    "    for action in range(agent.action_dim):\n",
    "        Q[(state, action)] = q_values[action]\n",
    "\n",
    "# 渲染Q值\n",
    "env.render_q(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入保存的模型\n",
    "checkpoint = torch.load(os.path.join(traning_data_root, 'dqn_gridworld_model_final.pth'))\n",
    "agent.online_net.load_state_dict(checkpoint['online_net_state_dict'])\n",
    "agent.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "agent.epsilon = checkpoint['epsilon']\n",
    "\n",
    "# 测试函数\n",
    "def test_policy(env, agent, height, width, num_tests=1000):\n",
    "    steps_count = list()\n",
    "    max_step = 50\n",
    "    success_count = 0\n",
    "    for i in range(num_tests):\n",
    "        raw_state = env.reset()\n",
    "        state = enhanced_one_hot_encode(raw_state, height, width, env.goal_state)\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        while not done and step_count < max_step:\n",
    "            action = agent.get_action(state)\n",
    "            raw_next_state, reward, done = env.step(action)\n",
    "            next_state = enhanced_one_hot_encode(raw_next_state, height, width, env.goal_state)\n",
    "            \n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "\n",
    "            if done and reward > 0:\n",
    "                success_count += 1\n",
    "                steps_count.append(step_count)\n",
    "                break\n",
    "    \n",
    "    avg_step = np.mean(steps_count)\n",
    "\n",
    "    print(f\"\\nSuccess rate: {success_count/num_tests:.2%}\")\n",
    "    return success_count/num_tests, avg_step\n",
    "\n",
    "# 测试训练好的策略\n",
    "succ_rate, avg_step = test_policy(env, agent, height, width)\n",
    "print(\"模型完成率: \", succ_rate, \"平均完成步数: \", avg_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
